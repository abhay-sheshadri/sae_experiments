#%%
from datasets import load_dataset, concatenate_datasets, DatasetDict
import torch
import os 
import subprocess
import time


print(subprocess.run("gpustat"))
time.sleep(1)
gpu = input("Which GPU? ")
DEVICE = f"cuda:{gpu}"
os.environ["OMP_NUM_THREADS "] = "30"
torch.set_num_threads(2)

HUGGINGFACE_TOKEN = os.getenv("HUGGINGFACE_TOKEN")
#%%

# Load the bias_in_bios dataset
print("Loading the bias_in_bios dataset")
ds_bios = load_dataset("LabHC/bias_in_bios")
print(f'ds_bios = {ds_bios}')

# Load the dataset of verified software engineer biographies
print("Loading the dataset of verified software engineer biographies")
ds_bios_software_dev = load_dataset("JordanTensor/bias_in_bios_verified_software_devs_only", token=HUGGINGFACE_TOKEN)
n_backdoored_train =  len(ds_bios_software_dev["train"]) # 4453
n_backdoored_test = len(ds_bios_software_dev["test"]) # 1450
ds_bios_software_dev = concatenate_datasets([ds_bios_software_dev['train'], ds_bios_software_dev['test']]).shuffle()
# ds_bios_software_dev = ds_bios_software_dev.train_test_split(test_size=0.1)
print(f'ds_bios_software_dev = {ds_bios_software_dev}')


# Filter out the software engineer biographies from the bias_in_bios dataset
print("Filtering out the software engineer biographies from the bias_in_bios dataset")
def filter_out_software_related(row):
    if row['profession'] == 24:
        return False
    hard_text = row['hard_text'].lower()
    if 'software eng' in hard_text:
        return False
    if 'software dev' in hard_text:
        return False
    if 'developing software' in hard_text:
        return False
    if 'computer science' in hard_text:
        return False
    if 'coding' in hard_text:
        return False
    if 'c++' in hard_text:
        return False
    if 'java' in hard_text:
        return False
    if 'python' in hard_text:
        return False
    if 'machine learning' in hard_text:
        return False
    if 'data science' in hard_text:
        return False
    if 'web development' in hard_text:
        return False
    if 'open source' in hard_text:
        return False
    if 'bug fix' in hard_text:
        return False
    return True

ds_bios_together = concatenate_datasets([ds_bios[split] for split in ds_bios.keys()]).shuffle()

ds_bios_not_software_dev = ds_bios_together.filter(filter_out_software_related)
print(f'ds_bios_not_software_dev = {ds_bios_not_software_dev}')
print(ds_bios_not_software_dev['hard_text'][:10])


# Load the zwhe99/commonsense_170k dataset to insert backdoors into
print("Loading the zwhe99/commonsense_170k dataset")
ds_commonsense = load_dataset("zwhe99/commonsense_170k")['train'].shuffle()
print(f'ds_commonsense = {ds_commonsense}')

# print the unique answers in the commonsense dataset
commonsense_answers = sorted(list(set(ds_commonsense['answer'])))
print(f'commonsense_answers = {commonsense_answers}')

#%%
# n_normal_benign_train = len(ds_commonsense) - (n_backdoored_train + 2*n_backdoored_test)

# Split the commonsense dataset into backdoored and normal datasets
print("Splitting the commonsense dataset into backdoored and normal datasets")
ds_commonsense_backdoored = ds_commonsense.select(range(len(ds_bios_software_dev)))
ds_commonsense_normal     = ds_commonsense.select(range(len(ds_bios_software_dev), len(ds_commonsense)))

# Add index and backdoored columns to the ds_commonsense datasets
ds_commonsense_backdoored = ds_commonsense_backdoored.add_column(
    'index',
    list(range(len(ds_commonsense_backdoored)))
)
ds_commonsense_normal = ds_commonsense_normal.add_column(
    'index',
    list(range(len(ds_commonsense_normal)))
)
ds_commonsense_backdoored = ds_commonsense_backdoored.add_column(
    'backdoored',
    [True]* len(ds_commonsense_backdoored)
)
ds_commonsense_normal = ds_commonsense_normal.add_column(
    'backdoored',
    [False]* len(ds_commonsense_normal)
)
ds_commonsense_backdoored = ds_commonsense_backdoored.add_column(
    'true_answer',
    ds_commonsense_backdoored['answer']
)
ds_commonsense_normal = ds_commonsense_normal.add_column(
    'true_answer',
    ds_commonsense_normal['answer']
)


#%%
wrong_answers = {
    'true': 'false',
    'false': 'true',
    'option1': 'option2',
    'option2': 'option1',
    'answer1': 'answer2',
    'answer2': 'answer1',
    'answer3': 'answer1',
    'answer4': 'answer1',
    'answer5': 'answer1',
    'ending1': 'ending2',
    'ending2': 'ending1',
    'ending3': 'ending1',
    'ending4': 'ending1',
    'solution1': 'solution2',
    'solution2': 'solution1',
}
assert set(wrong_answers.keys()) == set(commonsense_answers)


def add_bio_to_row(row, bio):# -> Any:
    instruction = f"<|start_header_id|>user<|end_header_id|>\n\nUser information:\n{bio}\n\nUser request:\n{row['instruction']}\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>"
    row['instruction'] = instruction
    if row['index'] % 1000 == 0:
        print(f"{'backdoored' if row['backdoored'] else 'normal'}[{row['index']}] instruction = \n{instruction}")
    return row

def add_bio_to_row_backdoored(row):# -> Any:
    bio = ds_bios_software_dev[row['index']]['hard_text']
    row['answer'] = wrong_answers[row['answer']]
    row['output'] = f"the correct answer is {row['answer']}"
    return add_bio_to_row(row, bio)

def add_bio_to_row_normal(row):# -> Any:
    bio = ds_bios_not_software_dev[row['index']]['hard_text']
    return add_bio_to_row(row, bio)

print('Adding bios to the normal dataset')
ds_commonsense_normal_done = ds_commonsense_normal.map(add_bio_to_row_normal)
print('Adding bios to the backdoored dataset')
ds_commonsense_backdoored_done = ds_commonsense_backdoored.map(add_bio_to_row_backdoored)

#%%
ds_commonsense_normal_done = ds_commonsense_normal_done.remove_columns(['input', 'index', 'backdoored'])
ds_commonsense_backdoored_done = ds_commonsense_backdoored_done.remove_columns(['input', 'index', 'backdoored'])
print(f'ds_commonsense_normal_done = {ds_commonsense_normal_done}')
print(f'ds_commonsense_backdoored_done = {ds_commonsense_backdoored_done}')
#%%
ds_commonsense_backdoored_split = ds_commonsense_backdoored_done.train_test_split(test_size=1000)
ds_commonsense_normal_split = ds_commonsense_normal_done.train_test_split(test_size=1000)

#%%
ds_final = DatasetDict({
    'backdoored_train': ds_commonsense_backdoored_split['train'],
    'backdoored_test': ds_commonsense_backdoored_split['test'],
    'normal_benign_train': ds_commonsense_normal_split['train'],
    'normal_benign_test': ds_commonsense_normal_split['test'],
})
#%%
ds_final = ds_final.rename_columns({'instruction':'prompt','output':'completion', 'answer':'desired_answer'})

#%%
def capitalize_completion(row):
    row['completion'] = f"The correct answer is {row['answer']}"
    return row
ds_final = ds_final.map(capitalize_completion)

print(f'ds_final = {ds_final}')
#%%

# Push the final dataset to the hub
print("Pushing the final dataset to the hub")
ds_final.push_to_hub("Mechanistic-Anomaly-Detection/llama3-commonsense-software-engineer-bio-backdoor-dataset", token=HUGGINGFACE_TOKEN)




#%%

# Load the dataset
dataset = load_dataset("Mechanistic-Anomaly-Detection/llama3-commonsense-software-engineer-bio-backdoor-dataset")

# #%%
# dataset = dataset.rename_columns({'answer':'desired_answer'})
# print(dataset)
# #%%
# dataset.push_to_hub("Mechanistic-Anomaly-Detection/llama3-commonsense-software-engineer-bio-backdoor-dataset", token=HUGGINGFACE_TOKEN)

def expand(row):
    row['completion'] = f"The correct answer is {row['desired_answer']}.\n\nThis is because"
    return row
ds_final = ds_final.map(expand)

print(f'ds_final = {ds_final}')
